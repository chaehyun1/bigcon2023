{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ba5b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc537ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa881c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d2c3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c3777e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/chaeh/bigcon/dataset/0910 착과수_컬럼추가.csv') #착과수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01e55212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['EXTN_TPRT', 'ABSLT_HMDT', 'EXTN_SRQT', 'WATER_LACK_VL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8dc44a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26064, 9)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cdf3deaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STRG_DT', 'FRST_TREE_CNT', 'PFBS_NTRO_CBDX_CTRN', 'SPL_TPRT_1',\n",
       "       'HTNG_TPRT_1', 'SPL_TPRT_2', 'AVE_INNER_HMDT_1_2',\n",
       "       'SKLT_OPDR_RATE_1_RIGHT', 'GDD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67c1e6",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87992f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d177844210>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0) #학습을 다시 수행 시 실헙을 동일하게 진행하기 위해서 난수를 동일하게 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af513ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d0c6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 초기화\n",
    "data_dim = 8 #전체 feature 개수\n",
    "output_dim = 1\n",
    "batch_size = 128 #배치 사이즈도 하이퍼파라미터로 설정해야 하지만, dataloader시 필요하여 임으로 지정함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d7cbef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 데이터 셋 생성\n",
    "def make_dataset(data, seq_len):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    \n",
    "    #sequence 데이터 생성\n",
    "    for i in range(len(data)-seq_len):\n",
    "        x = data[i:i+seq_len, :] #0~143, 1~144, ...\n",
    "        y = data[i+seq_len, [-1]] #0~143, 1~144, ...\n",
    "        data_X.append(x)\n",
    "        data_Y.append(y)\n",
    "    \n",
    "    return np.array(data_X), np.array(data_Y)\n",
    "\n",
    "def data_result(df, seq_length):\n",
    "    #필요한 컬럼만 선택\n",
    "    df = df[['PFBS_NTRO_CBDX_CTRN', 'SPL_TPRT_1', 'HTNG_TPRT_1',\n",
    "           'SPL_TPRT_2', 'AVE_INNER_HMDT_1_2', 'SKLT_OPDR_RATE_1_RIGHT', 'GDD', 'FRST_TREE_CNT']]\n",
    "\n",
    "    #train/test 분리 + validation\n",
    "    train_size = int(len(df)*0.6)\n",
    "    val_size = int(0.2 * len(df))\n",
    "    test_size = len(df) - train_size - val_size\n",
    "    train_set = df[0:train_size]\n",
    "    val_set = df[train_size:train_size+val_size]\n",
    "    test_set = df[train_size+val_size:] #train_size - seq_length\n",
    "    \n",
    "    #scaling\n",
    "    train_set = train_set.copy() #deepcopy 원리\n",
    "    val_set = val_set.copy()\n",
    "    test_set = test_set.copy()\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(train_set.iloc[:,:-1])\n",
    "\n",
    "    train_set.iloc[:,:-1] = scaler_x.transform(train_set.iloc[:,:-1])\n",
    "    val_set.iloc[:,:-1] = scaler_x.transform(val_set.iloc[:,:-1])\n",
    "    test_set.iloc[:,:-1] = scaler_x.transform(test_set.iloc[:,:-1])\n",
    "\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(train_set.iloc[:,[-1]])\n",
    "\n",
    "    train_set.iloc[:,-1] = scaler_y.transform(train_set.iloc[:,[-1]])\n",
    "    val_set.iloc[:,-1] = scaler_y.transform(val_set.iloc[:,[-1]])\n",
    "    test_set.iloc[:,-1] = scaler_y.transform(test_set.iloc[:,[-1]])\n",
    "\n",
    "    trainX, trainY = make_dataset(np.array(train_set), seq_length) #sequence 데이터로 생성\n",
    "    valX, valY = make_dataset(np.array(val_set), seq_length)\n",
    "    testX, testY = make_dataset(np.array(test_set), seq_length)\n",
    "    \n",
    "    #사용하는 device로 적용, numpy를 tensor로 바꾸기 \n",
    "    trainX_tensor = torch.FloatTensor(trainX).to(device)\n",
    "    trainY_tensor = torch.FloatTensor(trainY).to(device)\n",
    "\n",
    "    valX_tensor = torch.FloatTensor(valX).to(device)\n",
    "    valY_tensor = torch.FloatTensor(valY).to(device)\n",
    "\n",
    "    testX_tensor = torch.FloatTensor(testX).to(device)\n",
    "    testY_tensor = torch.FloatTensor(testY).to(device)\n",
    "    \n",
    "    #TensorDataset은 학습 데이터 X와 레이블 Y를 묶어 놓은 컨테이너\n",
    "    dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "    dataset_val = TensorDataset(valX_tensor, valY_tensor)\n",
    "\n",
    "    #DataLoader를 통해 배치 크기, 데이터를 섞을지 등을 결정한다.\n",
    "    #참고) drop_last: batch_size에 따라 마지막 batch의 길이가 달라질 수 있어서, 마지막 배치를 사용할지 여부\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False, drop_last = True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size = batch_size, shuffle = False, drop_last = True) \n",
    "    \n",
    "    return dataloader, dataloader_val, testX_tensor, testY_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa47227",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "961e0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 만들기\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_length, output_dim, layers): \n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        #참고) batch_first: True이면 Output 값의 사이즈는 (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers = layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, bias = True)\n",
    "    \n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        #hidden state 초기화 함수\n",
    "        #매번 새로운 학습을 하여 최적의 값을 찾기 위해서 사용\n",
    "        self.hidden = (\n",
    "        torch.zeros(self.layers, self.seq_length, self.hidden_dim),\n",
    "        torch.zeros(self.layers, self.seq_length, self.hidden_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9197b1d",
   "metadata": {},
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7357870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f38bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_dim': [64, 128, 256],\n",
    "    'seq_length': [36, 72, 144],\n",
    "    'layers': [1, 2, 3],\n",
    "    'lr': [0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c5804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gridsearch start: 100%|███████████████████████████████████████████████████████████| 81/81 [24:18:30<00:00, 1080.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_dim': 256, 'layers': 1, 'lr': 0.001, 'seq_length': 144}\n",
      "Best Validation MSE: 0.25332796162734894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for params in tqdm(ParameterGrid(param_grid), desc = 'gridsearch start'):\n",
    "    dataloader, dataloader_val, testX_tensor, testY_tensor = data_result(df, params['seq_length'])\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = LSTMModel(8, params['hidden_dim'], params['seq_length'], 1, params['layers'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 모델 학습 및 평가\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, samples in enumerate(dataloader):\n",
    "            x_train, y_train = samples\n",
    "            \n",
    "            #seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            #h(x) 계산\n",
    "            outputs = model(x_train)\n",
    "            \n",
    "            #cost 계산\n",
    "            loss = criterion(outputs, y_train)\n",
    "            \n",
    "            #참고: 파이토치에서 역전파 수행 시 계속 값을 더해주기 때문에,\n",
    "            #역전파 수행 전 gradients를 0으로 만들어주고 시작해야 한다.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 평가 루프\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        \n",
    "        for batch_idx, samples in enumerate(dataloader_val):\n",
    "            with torch.no_grad():\n",
    "                x_val, y_val = samples\n",
    "                model.reset_hidden_state()\n",
    "                output = model(x_val)\n",
    "            predictions.extend(output.tolist())\n",
    "            targets.extend(y_val.tolist())\n",
    "        mse = mean_squared_error(targets, predictions)\n",
    "\n",
    "    # 현재 하이퍼파라미터 조합의 MSE를 기록\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation MSE:\", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74dca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
